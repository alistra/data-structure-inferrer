\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[colorinlistoftodos, textwidth=2cm, shadow]{todonotes}
\usepackage{listings}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\lstset{language=C}


%\usepackage[]{polski}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{#4}{#1}}
\newcommand{\high}{\mathrm{high}}
\newcommand{\low}{\mathrm{low}}

\newtheorem{theorem}{Twierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{observation}[theorem]{Obserwacja}
\newtheorem{definition}[theorem]{Definicja}
\newtheorem{fact}[theorem]{Fakt}
\newtheorem{assumption}[theorem]{Założenie}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\title{\emph{dsinf}: Source Based Data Structure Inference}
\author{Aleksander Balicki}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	When you need to store data, \todo{Connect the parts of the sentence} most of popular languages today already have libraries with all the important data
	structures implemented.  Programmers just have to be taught when to use them.  Some of the cases of choosing the right data structure look sufficiently
	easy, so a computer could do it automatically. This work describes the \emph{dsinf} project, a framework for inferring the best data structure matching
	your task, based on the program's source code in C language. \missingfigure{ result, conclusion, http://www.ece.cmu.edu/~koopman/essays/abstract.html}
\end{abstract}

\pagebreak

\tableofcontents

\vfill

\section{Introduction}
	\missingfigure{Introduction here}

\section{Data structure inference}

	\subsection{Comparison of the complexities}
		When trying to define some kind of ordering on data structures - so we can decide which is best at the moment - we encounter a problem.  We want
		to compare the asymptotical complexities of operations on data structures.  We can't do it for the general case, because it's too complicated,
		because an arbitrary function can describe the complexity of an operation.

		Asymptotical complexity of an operation we store as a pair of type:
		\begin{eqnarray}
			AsymptoticalComplexity = Int \times Int,
		\end{eqnarray}
		where
		\begin{eqnarray}
			(k, \; l) \; means \; O(n^k \log^l{ n}).
		\end{eqnarray}
		The reason to choose such a type is that it's easier to compare than the general case (we can do a lexicographical comparison of the two
		numbers) and it distincts most of the data structure operation complexities.

		Sometimes we have to use some qualified complexities:
		\begin{eqnarray}
			ComplexityType = \{ Normal, \; Amortized, \; Amortized \;Expected, \; Expected \}
		\end{eqnarray}

		The overall complexity can be seen as a type:
		\begin{eqnarray}
			Complexity = AsymptoticalComplexity \times ComplexityType
		\end{eqnarray}
		Here we can also use a lexicographical comparison, but we have to say that
		\begin{eqnarray}
			Amortized < Normal,\\
			Expected < Amortized,\\
			Amortized \; Expected < Expected
		\end{eqnarray}
		and that $<$ is transitive.

		We also always choose the smallest asymptotic-complexity-wise complexity.  For example, we have a search operation on a splay tree. It's $O(n)$,
		but $O(\log n)$ amortized, so it's represented as $((0,1),Amortized)$.
	\subsection{Choosing the best data structure}
		We define a set $DataStructureOperations$. We can further extend this set, but for now assume that
		\begin{eqnarray}
		  	DataStructureOperations = \{Insert, \; Update, \; Delete, \; FindMax,\; DeleteMax, \; \dots\}.
		\end{eqnarray}
		Each of the $DataStructureOperations$ elements symbolizes an operation you can accomplish on a data structure.

		The type
		\begin{equation}\label{data-structure-type}
			DataStructure = DataStructureOperations \rightarrow Complexity
		\end{equation}
		represents a data structure and all of the operations implemented for it, with their complexities, as a partial function from
		DataStructureOperations to Complexities.

		When trying to find the best suited data structure for a given program $P$, we look for data structure uses in $P$. Let
		\begin{equation}\label{dsu-type}
			DSU(P) :: P(DataStructureOperations)
		\end{equation} be the set of data structure operations, that are used somewhere in the source code of $P$.

		We define a parametrized comparison operator for data structures $<_{DSU(P)}$ defined as:
		\begin{center}
			\begin{equation}
				d_1 <_{DSU(P)} d_2
			\end{equation}
				$\Updownarrow$
			\begin{equation} \label{data-structure-order}
				|\{(o, c_1) \in d_1 | o \in DSU(P) \wedge (o,c_2) \in d_2 \wedge c_1 < c_2 \}| <
				0.5 * \lfloor |DSU(P)| \rfloor
			\end{equation}
		\end{center}
		If a data structure implements more operations 'faster' than we choose that structure over the other one.


		If we fix P, we have a preorder on data structures induced by $<_{DSU(P)}$ and we can sort those data structures using this order. The maximum
		element is the best data structure for the task.
	\subsection{Collecting the program data}
		\label{dsu-definition}
		\subsubsection{C API}
		\subsubsection{Analysis}

\pagebreak

\section{Extensions of the idea}
	\subsection{Second extremal element}
		If we want to find the maximal element in a heap, we just look it up in $O(1)$, that's what heaps are for.  If we want to find the minimal
		element we can modify the heap, for it to use an order, which would allow us to lookup the minimal element in $O(1)$.  What happens if we want
		to find the max and the min element in the duration of one program?  How to modify our framework to handle this kind of situations?
		  	$$DataStructureOperations = \{\dots, \; FindFirstExtremalElement,\\$$
			$$	DeleteFirstExtremalElement,\\$$
			$$	FindSecondExtremalElement,\\$$
			$$	DeleteSecondExtremalElement, \; \dots\}.$$
		Now we can add two complexity costs to the data structure definition. We can always reverse the order, so the cheaper one can be used primarily,
		and the more expensive one in situations when we need both types of extremal elements.
	\subsection{Detecting importance of an operation}
		Detecting the importance of a single data structure operation is an important problem, mainly because it's undecidable.  This program shows that
		the problem of compile-time deciding on the best data structure is impossible to solve.
		\lstinputlisting{thesis-pics/undecidable.c}
		In the above example, the best data structure is depending on the user input, which is not known at compile time.

		Also code that is not totally dependent on user input can cause problems to analyze.
		\lstinputlisting{thesis-pics/weighted-instructions.c}
		Here we see a very costly instruction used only once, and a few instructions run in a loop for a few million times.  To anyone that knows
		program complexities it's obvious that this one heavy instruction doesn't affect the execution time of the whole program, yet the framework at
		the current state treats those instructions equally.

		\subsubsection{Code pragmas}
			A possible soultion to this problem is to let the programmer add code pragmas to his source code, so he decides how important an
			instruction is in relation to other instructions and then the framework makes use of those values in choosing the data structure.
			\lstinputlisting{thesis-pics/code-pragmas.c}
			In the above example programmer can add a weights to the operations, assigning very low values for statements that are used rarely or
			for debug purposes, and very high values for crucial parts of the program.

			There would be an API change needed.
			\lstinputlisting{thesis-pics/code-pragmas-api-change.c}
			This isn't the perfect solution, because we still need the programmer to judge which operations should have high weights, but it's nice
			when a programmer wants to use it for debug purposes or otherwise tinker with it.

		\subsubsection{Choosing the best data structure with weigths}
			We need to change the algorithm for choosing the best data structure, for it to handle weights to the operations in the source code. We
			change the type of the $DSU()$ operation from
			\ref{dsu-type} to:
			\begin{equation}
				DSU_w(P) :: DataStructureOperations -> Int
			\end{equation}
			This change introduces weights for operations in program $P$. We can use a partial function, or just use $0$ as a value for operations
			that do not occur in the program. We always want to get the highest weight from all the weighted operation instances in the program, so
			the $DSU()$ definition also needs a change from \ref{dsu-definition}:
			\begin{equation}
				DSU_w(P) =
			\end{equation}

			We change the definition of \autoref{data-structure-order} to:
			\begin{equation}
			\end{equation}

		\subsubsection{Profile-guided optimization}
			Profile-guided optimization (PGO) is a compiler optimization technique in computer programming to improve program runtime performance.
			In contrast to traditional optimization techniques that solely use the source code, PGO uses the results of test runs of the
			instrumented program to optimize the final generated code \todo{reference wikipedia}.

			If the user has some test data, he can run against his program, we can take advantage of that. First we choose the best data structure
			with an unmodified method and link some library to the executable. Of course this won't be the best data structure possible. Then user
			can run the test suite with code coverage option, like \emph{gcov} in GCC, turned on in the compiler.
			\missingfigure{Example source code}
			\missingfigure{Example gcov file}
			Then the user can pass the \emph{gcov} generated files and the source code to the framework again. The framework can extract line hits
			from the \emph{gcov} files on the data structure operations and set weights on the operations according to the extracted data and then
			use the choosing algorithm for operations with weigths.

			It's a better solution than letting the user set the weights himself, but still, the data the inference is based upon, comes from tests
			and there's no guarantee the real world data will match the test data.

		\subsubsection{Transforming datastructures on-line}
	\subsection{Generic data structure modifications}

		max elem cache/linked leaves/functor in ocaml
	\subsection{Different element types}
		Currently the framework works only for integer elements. We could extend it to every primitive type in C, but it would require some changes.
		Some data structures require the type to be comparable, which wouldn't be a big problem, because there is a comparison semantics defined on those
		types. There's also a hash function needed, because some data structures, like a hashtable, need to compute hash values to work. We can just use
		the binary representation of other types, and use it as an integer for the hash function. This whole modification doesn't need any user
		interaction.

		There's a bigger problem with composite types. Comparing pointers isn't obvious. You may want to compare addresses or the contents under that
		address, depending if you want object or structural equality. The same problem is with hash function arguments. There's also a problem with
		possible memory leaks. You can pass a pointer to a string to the data structure and lose the pointer in the program, then when the structure
		deletes the pointer and the string stays in the memory to the program's end. Adding this to the framework would require passing the comparison
		function, hash function and some kind of destructor function to the data structure, or possibly use some reference counting system. It would be
		very technical and would be beyond the scope of this thesis. With an array or a big struct passed to the framework, there's a problem if we want
		to share the datastructure and just copy the pointer, copying the whole thing may be a bad idea, because it can have quite a lot of data inside.

		\subsubsection{Linked data structures}
			When we want to store structs like this:
			\lstinputlisting{thesis-pics/linked-struct.c}
			Our framework, at the moment, could generate some data structure for the structs.
			\lstinputlisting{thesis-pics/struct-old.c}
			It would be nice if our program enabled things like:
			\lstinputlisting{thesis-pics/struct-new.c}
	\subsection{Upper bound on the element count}
		so we can choose between malloc and static allocation
	\subsection{Minimal element count treshold}
		It's worth noticing that we compare only the asymptotical complexity of data structures. Some awfully complicated structures can have good
		asymptotical results, but the constant is quite high. We can avoid this problem by setting a treshold for each structure, what is the smallest
		number of elements to use this data structure.

		Another problem arises, how to know at compile time, how many elements a data structure will have at runtime. We can ask the user to explicitely
		specify the number during compilation or we can try to detect how big the declared data is.
	\subsection{Comparing a database}
		\label{sub:database}
\section{Program}
	\subsection{Recommendation mode}
		prints recommendations
	\subsection{Advice mode}
		prints advice
	\subsection{Compile mode}
		linkes appropriate lib

\end{document}
% vim: set wrap tw=160:
