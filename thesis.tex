\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[colorinlistoftodos, textwidth=2cm, shadow]{todonotes}
\usepackage{listings}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage[section]{placeins}
\lstset{language=C}

%\usepackage[]{polski}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in
\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\title{\emph{dsinf}: Source Based Data Structure Inference}
\author{Aleksander Balicki}
\date{\today}

\begin{document}

\vfill

\maketitle

\begin{abstract}

    When you need to store data in a data structure, most of popular languages today already have libraries with
    convenience data structures implemented. Programmers just have to be taught when to use a data structure. Some of
    the cases of choosing the right data structure look sufficiently easy, so a program could do it automatically. A
    novice programmer doesn't have to know what are those data structures for, he can just use this framework.  This
    work describes the \emph{dsinf} project, a framework for inferring the best data structure matching the task, based
    on the program's source code in C language. \missingfigure{result, conclusion,
    http://www.ece.cmu.edu/~koopman/essays/abstract.html}

\end{abstract}

\pagebreak

\tableofcontents

\vfill

\section{Introduction} \label{sec:intro}
	In computer science, a data structure is a particular way of storing and organizing data in a computer so that
	it can be used efficiently\cite{Wids}.

	The interface of a data structure is a set of operation that you can perform on a data structure and some
	semantics of those operations.

	The implementation of a data structure is a realization of some data structure interface in some programming
	language, that is consistent with the semantics.

	We can distinguish a lot of different types of data structures:
	\begin{itemize}
		\item Map/Associative array/Dictionary is a data structure for storing $(key,\;value)$ pairs, it allows
			insertion, deletion, modification and lookup
		\item Multimap is the same as Map, but with the possibility of storing multiple values for a one key
		\item List is a data structure for storing ordered sets of values
		\item Set is a data structure for storing unordered sets of values
		\item Multiset is the same as Set, but with the possibility of storing multiple objects that are equal
			under some equivalence relation
		\item Queue is a data structure implementing operations for queuing and dequeuing elements
		\item Deque is a queue, where you can add and remove elements from both ends as opposing to one
		\item Stack is a data structure implementing operations for putting something on the top of the stack or
			removing the top element from it
		\item Priority queue is like a regular queue, but each element has a priority associated with it, which
			is used for ordered dequeuing of the elements
		\item String is a data structure for storing text
		\item Tree is a data structure, to hold some tree-like hierarchical structure
		\item Graph is a data structure, to store elements and a relation on those elements
		\item Others, which usually are a mix of those previous ones
	\end{itemize}

	The different implementation of those data structures make it possible, that specific operations run time is
	lower than the others, so we can't use one data structure, for all the tasks.

	\subsection{What is dsinf?} \label{sub:intro}

		dsinf is a source code analyzer. It works by analyzing specially prepared C code, that C code for every
		instance of a data structure uses a type defined in the dsinf library, we will call the type a data
		structure wildcard. The type represents a data structure with the most general interface, i.e. a set sum
		of interfaces of all data structures available in the dsinf framework. It connects the data structures
		and operations performed on them and then reasons about which of the available data structure
		implementations would be best for the instances of data structure wildcards in your code, finally (if
		possible) choosing the best matches and printing them out or linking them to your code to create a
		working binary.

		The analysis now works after preprocessing, before the actual compilation. In the future it can be
		pushed to the phase between compilation and linking as described in \autoref{sec:future}.

		The analysis process uses asymptotic complexity for data structure comparison. The process can also be
		modified by providing some source code annotations or you can use test data to tailor your data
		structure as described in \autoref{sub:importance}.

		The idea of the framework is to provide a data structure, that when used is always good enough. Due to
		unsolvability of the general case of the problem, the effect always will be worse than a hand tailored
		data structure to your task, but can save up some time (both programming time and execution time) if we
		need to write something fast. To reiterate, using dsinf for the most critical bottleneck of your
		application probably isn't a good idea.

	\subsection{Comparing to a database} \label{sub:database}
		The main difference between dsinf and a database is that a database has to implement all the operations
		for all the possible SQL queries, including some crosschecks, counting, summations, extremal elements
		and with dsinf your data structure implementation doesn't have to be ready for all the possible SQL
		queries that can be constructed, but only the ones you used in your source code, which enables it to
		potentially gain some performance over a database.

	\subsection{Comparing to class clusters} \label{sub:classcluster}
		A class cluster is an architecture that groups a number of private, concrete subclasses under a public,
		abstract superclass. The grouping of classes in this way provides a simplified interface to the user,
		who sees only the publicly visible architecture. Behind the scenes, though, the abstract class is
		calling up the private subclass most suited for performing a particular task\cite{AppleCC}. One could
		have an object representing e.g. an array, and have a lot of different private implementations for it, like
		large, sparse arrays, or arrays with sizes known at compile time and optimize operations for those
		cases. The downside of such method is that you can alternate between the data structure representations
		that need a lot of time to transform into each other. dsinf knows exactly what operations can occur and
		tailors the data structure to the operations that will happen.

	\subsection{C API}
        The framework detects specific C functions in the source code and bases the inference on them. Now we define,
        what are those functions. Our first api is on the \autoref{fig:C-api-v1}.

        \begin{figure}[h!]
            \lstinputlisting{thesis-pics/ds1.h}

            \caption{The dsinf C API}

            \label{fig:C-api-v1}
        \end{figure}

        There is a problem with the \autoref{fig:C-api-v1} API, if we want to use an operation on a data structure that
        is asymptoticly faster than the speed of a lookup of an element, we simply don't know how to do it. Let's say we
        have a balanced search tree that guarantees $O(log n)$ lookups, with a linked list of all the elements (like in
        \autoref{sec:gdsm:le}) to enhance the speed of the successor and predecessor operations to $O(1)$. If we wanted to create a
        successor function using this API convention, it would look like this:

        \begin{lstlisting}
dstype successor_d(ds, dstype);
        \end{lstlisting}
        The problem with this function is that it doesn't take a pointer to the element in a data structure, so it has
        to actually find the value given in the actual parameter of the function, and then compute the successor in
        $O(1)$, but the search takes $O(log n)$, so the whole operation takes $O(log n)$, which is bad.

        To fix this we declare a new type for an data structure element. The type encompasses a pointer to the place in
        he data structure, from where we can start our search, so we don't have to waste time on finding the element.

        This is the second version of the API, updated to use the $dselem$ struct, instead of a plain value is on the
        \autoref{fig:C-api-v2}.

        \begin{figure}[h!]
            \lstinputlisting{thesis-pics/ds2.h}

            \caption{The better dsinf C API}

            \label{fig:C-api-v2}
        \end{figure}

        The following code examples will sometimes use the \autoref{fig:C-api-v1} API convention for simplicity.

\section{Data structure inference}

	\subsection{Comparison of the complexities}

		If we want to find the best possible data structure for a task, we have to define some kind of ordering
		on the data structures, so we can judge which structures are better than others.  We want to compare the
		asymptotic complexities of operations on data structures. When trying to define such ordering, we
		encounter a problem.  We can't do it for the general case, because an arbitrary function can describe
		the complexity of an operation and a comparison of asymptotic complexities is complicated.

		Asymptotic complexity of an operation we store as a pair of type:

		\begin{eqnarray}
			AsymptoticalComplexity = Int \times Int,
		\end{eqnarray}

		where

		\begin{eqnarray}
			(k, \; l) \; means \; O(n^k \log^l{ n}).
		\end{eqnarray}

		The reason to choose such a type is that it's easier to compare than the general case (we can do a
		lexicographical comparison of the two numbers) and it distinguishes most of the data structure operation
		complexities.

		Sometimes we have to use some qualified complexities:

		\begin{eqnarray}
			ComplexityType = \{ Normal, \; Amortized, \; Amortized \;Expected, \; Expected \}
		\end{eqnarray}

		The overall complexity can be seen as a type:

		\begin{eqnarray}
			Complexity = AsymptoticalComplexity \times ComplexityType
		\end{eqnarray}

		Here we can also use a lexicographical comparison, but we have to say that

		\begin{eqnarray}
			Amortized < Normal,\\
			Expected < Amortized,\\
			Amortized \; Expected < Expected
		\end{eqnarray}

		and that $<$ is transitive.

		The ordering is done in such way, because normal complexity of $O(f(n))$ guarantees that no operation
		will exceed that time multiplied by a constant. Amortized is worse, because here, we can only guarantee
		that $n$ consecutive operations will have the average time $O(f(n))$, but single operations can spike
		heavily. Expected is worse than amortized, because here we can only have some probability of achieving
		the mentioned complexity for an operation and there's no bound on the number of spikes, which were under
		control in an amortized complexity.

		We also always choose the smallest asymptotic-complexity-wise complexity.  For example, we have a search
		operation on a splay tree. It's $O(n)$, but $O(\log n)$ amortized, so it's represented as
		$((0,1),Amortized)$.

	\subsection{Choosing the best data structure} \label{sec:choose-ds}

		We define a set $DataStructureOperation$. We can further extend this set, but for now assume that

		\begin{eqnarray}
			DataStructureOperation = \{Insert, \; Update, \; Delete, \; FindMax,\; DeleteMax, \; \dots\}.
		\end{eqnarray}

		Each of the $DataStructureOperation$ elements symbolizes an operation you can accomplish on a data
		structure.

		The type

		\begin{equation}\label{data-structure-type}
			DataStructure = DataStructureOperation \rightarrow Complexity
		\end{equation}

		represents a data structure $ds$ and all of the operations implemented for it, with their complexities, as a
		partial function $f$ from DataStructureOperation to Complexities. It represents some implementation of a
		data structure, where existing arguments of $f$ are the operations, which are possible to accomplish on
		$ds$, and the value of the function represents asymptotic speed of the operation. For example in
		\autoref{tab:rbt} presents such representation for Red-Black Trees.

		\begin{table}[h!]
			\centering
			\begin{tabular}{|l|l|}
				\hline
				\multicolumn{2}{|c|}{Red-Black Trees} \\
				\hline
				Operation & Complexity \\
				\hline
				Insert 	        & $O(lg \; n)$ \\
				Update          & $O(lg \; n)$ \\
				Delete	        & $O(lg \; n)$ \\
				Find Max 	& $O(lg \; n)$/$O(1)$\\
				Delete Max	& $O(lg \; n)$ \\
				Search 		& $O(lg \; n)$ \\
				Successor 	& $O(lg \; n)$/$O(1)$\\
				\hline
			\end{tabular}
			\caption{A table showing the representation of Red-Black trees; complexity varies on
			implementation, more in \autoref{sub:gdsm}}
			\label{tab:rbt}
		\end{table}

		When trying to find the best suited data structure for a given program $P$, we look for data structure
		uses in $P$. Let

		\begin{equation} \label{dsu-type}
			DSU(P) :: [P(DataStructureOperation)]
		\end{equation}

		be a set of sets of data structure operations, that are used on one persistent data structure identity
		somewhere in the source code of $P$.

		For every $ds \in DSU(P)$, we define a parametrized comparison operator for data structures $<_{ds}$
		defined as:

		\begin{center}

			\begin{equation}
				d_1 <_{ds} d_2
			\end{equation}

			$\Updownarrow$

			\begin{equation} \label{data-structure-order}
				|\{(o, c_1) \in d_1 | o \in ds \wedge (o,c_2) \in d_2 \wedge c_1 < c_2 \}| < 0.5 *
				\lfloor |ds| \rfloor
			\end{equation}

		\end{center}

		The equation doesn't take into account the data structures that don't implement the needed operations,
		we discard them first.

		If a data structure implements more operations "faster" than we choose that structure over
		the other one.

		For a fixed P, we have a preorder on data structures induced by $<_{ds}$ and we can sort data structures
		available to the framework using this order. The maximum element is the best data structure
		implementation for the persistent data structure identity.

	\subsection{Collecting the program data} \label{dsu-definition}

		In \autoref{dsu-type} the type of the $DSU()$ operation was mentioned, this section shows, how $DSU()$ is
		defined. We're not considering the problem of 2 different variables having the same name and/or shadowing each
        other, to evade this we can just rename each variable to an UUID, and then remember the mapping for nicer
        output.

		Let $DSOPS(P) :: [(VariableName, DataStructureOperation)]$ be a set sum of all the pairs that:
		\begin{enumerate}

			\item \label{it:global} $\{(g, o)\; |\; g \texttt{ is a global data structure variable in $P$ }
				\wedge \\ o \texttt{ is an operation performed on $g$, somewhere in $P$ } \}$

			\item \label{it:auto} $\{(d, o)\; |\; d \texttt{ is a data structure declared somewhere } \\
				\texttt{ in a body of a function $f$ in $P$ } \wedge \\ o \texttt{ is an operation
				performed on $d$, somewhere in $f$ } \}$

			\item \label{it:param} $\{(p, o)\; |\; p \texttt{ is a formal parameter of data structure type,}
				\\ \texttt{of a function $f$ in $P$ } \wedge \\ o \texttt{ is an operation performed on
				$p$, somewhere in $f$ } \}$

		\end{enumerate}

        \begin{figure}[h]
            \lstinputlisting{thesis-pics/collecting_global.c}

            \caption{An example of using the global variables collection rule}

            \label{fig:global-collection-rule}
        \end{figure}
        \begin{figure}[h]
            \lstinputlisting{thesis-pics/collecting_declared.c}
            \caption{An example of using the declared variables collection rule}

            \label{fig:declared-collection-rule}
        \end{figure}
        \begin{figure}[h]
            \lstinputlisting{thesis-pics/collecting_function.c}

            \caption{An example of using the function parameters collection rule}

            \label{fig:parameter-collection-rule}
        \end{figure}

        \clearpage
 

		We want to group the elements in $DSOPS(P)$ to detect the persistent identities \cite{Okasaki} of
		data structures, meaning that in one group, there will be data structure operations conducted on one
		data structure from its allocation, to deallocation or the end of the program, counting in passing the structure as a
		parameter to another function or copying the pointer to the structure.

		To group the operations:
		\begin{itemize}

            \item \textbf{Persistent Identities} - for every two pairs $(d_1, o_1)$ and $(d_2, o_2)$ created using
                \autoref{it:global} or \autoref{it:auto}, we put them in the same group if $d_1 = d_2$.

            \item \textbf{Function calls} - for every pair $(p, o_1)$ that was created by using the \autoref{it:param}
                from function $f$, which was called with the actual data structure parameter $d$ as the formal parameter
                $p$, we put $o_1$ into the group of operations on $d$. For different persistent identities of data
                structures in function calls of $f$, we don't merge all the actual parameters' operations in one big
                group, just copy the operations to the according groups.

            \item \textbf{Copy propagation} - for every $(do, o_1)$ and $(dc, o_2)$, where the $dc$ variable was
                obtained by copying the value of the variable $do$, we put $o_1$ and $o_2$ in the same group.

            \item \textbf{Uniqueness} - for every group, we delete repeating elements

		\end{itemize}

        \begin{figure}[h]
            \lstinputlisting{thesis-pics/function-call-grouping.c}

            \caption{An example of using the function call grouping rule}

            \label{fig:function-call-grouping}
        \end{figure}

        \clearpage

		List of those groups is the $DSU(P)$. After this operation, each group has operations performed on the
		same persistent identity of a data structure. For every such group we find the best matching data
		structure like shown in \autoref{sec:choose-ds}.


\pagebreak

\section{Extensions of the idea}

	\subsection{Second extremal element}

		If we want to find the maximal element in a heap, we just look it up in $O(1)$, that's what heaps are
		for.  If we want to find the minimal element we can modify the heap, for it to use an order, which would
		allow us to lookup the minimal element in $O(1)$.  What happens if we want to find the max and the min
		element in the duration of one program?  How to modify our framework to handle this kind of situations?

		\begin{eqnarray*}
			DataStructureOperation = \{\dots, \; FindFirstExtremalElement,\\
			DeleteFirstExtremalElement,\\
			FindSecondExtremalElement,\\
			DeleteSecondExtremalElement\}.
		\end{eqnarray*}

        Now we can add two complexity costs to the data structure definition. We always try to map the first extremal
        element to the kind of element that is used more. We can always reverse the order, so the cheaper one can be
        used primarily, and the more expensive one in situations when we need both types of extremal elements.

	\subsection{Importance of operations} \label{sub:importance}

		Detecting the importance of a single data structure operation is an important problem, because a lot of
		programs will fall into a class where compile-time data structure selection is undecidable. The
        program in \autoref{fig:undecidable} shows that the problem of compile-time deciding on the best data structure
        is impossible to solve.

        \begin{figure}[!h]
			\lstinputlisting{thesis-pics/undecidable.c}

			\caption{An example of the problem of choosing the fastest data structure for a program being
			undecidable, user interaction}

			\label{fig:undecidable}
		\end{figure}

		In the \autoref{fig:undecidable}, the best data structure is depending on the user input, which is not known at
		compile time. If we analyze it as is, it will decide basing on the information, that every operation is
		used, so the final data structure won't be specific to the task, but just average at everything.

		The code that is not totally dependent on the user input can also cause problems to analyze.

        \begin{figure}[!h]
			\lstinputlisting{thesis-pics/weighted-instructions.c}

			\caption{An example of the problem of choosing the fastest data structure for a program being hard to
            analyze, no user interaction}

			\label{fig:hard-no-user}
		\end{figure}

		In the \autoref{fig:hard-no-user} we see a very costly instruction used only once
		($delete\_max\_d$), and a few instructions ($insert$, $search$) run in a loop for a few million times.
		To anyone that knows program complexities it's obvious that this one heavy instruction doesn't affect
		the execution time of the whole program, yet the framework at the current state treats those
		instructions equally and will probably choose something like Red-Black Trees, to minimize the time of
		the $delete\_max\_d$, instead of ignoring its cost and using a very fast Hashtable.

        We can formally show, that the problem of choosing the fastest data structure is undecidable, by showing that
        it's not easier than the halting problem. In \autoref{fig:undecidable-no-user} we run the $delete\_max\_d$ only
        if a turing machine that we defined accepts an input, so we don't know if we should choose a datastructure that
        implements $delete\_max\_d$ fast, or just ignore it, because it won't ever be used because of the machine not
        stopping the computation.

        \begin{figure}
			\lstinputlisting{thesis-pics/turing-machine.c}

			\caption{An example formally showing the undecidability of the problem, no user interaction}

			\label{fig:undecidable-no-user}
		\end{figure}

		\subsubsection{Code pragmas} \label{sec:pragmas}

			A possible solution to this problem is to let the programmer add code pragmas to his source
			code, so he decides how important an instruction is in relation to other instructions and then
			the framework makes use of those values in choosing the data structure.

            \begin{figure}[!h]
				\lstinputlisting{thesis-pics/code-pragmas.c}

				\caption{Source code with pragmas, that state the importance of the operation}

				\label{fig:code-pragmas}
			\end{figure}

            In the example on \autoref{fig:code-pragmas} programmer can add a weights to the operations, assigning very low values
			for statements that are used rarely or for debug purposes, and very high values for crucial
			parts of the program.

            We would have to extend the API like on the \autoref{fig:code-pragmas-api}.

            \begin{figure}[!h]
				\lstinputlisting{thesis-pics/dsw.h}

				\caption{dsinf API, using additional arguments for operation importance}

				\label{fig:code-pragmas-api}
			\end{figure}

			This isn't the perfect solution, because we still need the programmer to judge which operations
			should have high weights, but it's nice when a programmer wants to use it for debug purposes or
			otherwise tinker with it.

		\subsubsection{Choosing the best data structure with weights} \label{sec:choose-weights}

			We need to change the algorithm for choosing the best data structure, for it to handle weights
			to the operations in the source code. We change the type of the $DSU()$ operation from
			\autoref{dsu-type} to:

			\begin{equation}
				DSU_w(P) :: [(VariableName, DataStructureOperation, Int)]
			\end{equation}

			This change introduces weights for data structure operations in program $P$. We can use the
			additional $Int$ field to store specified weight of the operation.  For operations that are used
			multiple times, we sum the weights and the resulting sum is the value for that operation.

			The $DSU()$ definition needs a change from \autoref{dsu-definition}:

			\begin{itemize}
				\item Every rule in $DSOPS(P)$ now adds a triple, where the additional argument is
					weight obtained by using a method from \autoref{sec:pragmas} or
					\autoref{sec:pgo}

				\item The last step of grouping, that removed the same elements, now sums up the weights
					of the same operation elements and substitutes it with a new element with the
					sum as its weight
			\end{itemize}

            We change the definition of \autoref{data-structure-order} to:


            \begin{equation}
                d_1 <_{ds} d_2
            \end{equation}

            \begin{center}
                $\Updownarrow$
            \end{center}

            \begin{equation} \label{data-structure-order-weights}
                \displaystyle \sum \{w_1 - w_2| (o, c_1, w_1) \in d_1, o \in ds, (o,c_2, w_2) \in d_2, c_1 < c_2 \} > 0
            \end{equation}

            The \autoref{data-structure-order-weights} is the generalization of the previous equation, which was the
            instantiation of this equation with all the weights equal.


		\subsubsection{Profile-guided optimization} \label{sec:pgo}

			Profile-guided optimization (PGO) is a compiler optimization technique in computer programming
			to improve program runtime performance.  In contrast to traditional optimization techniques that
			solely use the source code, PGO uses the results of test runs of the instrumented program to
            optimize the final generated code \cite{Wipgo}.

			If the user has test data, he can run against his program, we can take advantage of that.  First
			we choose the best data structure using an unmodified method and link some library to the
			executable. Of course this won't be the best data structure possible. Then user can run the test
			suite with code coverage option, like \emph{gcov} in GCC, turned on in the compiler. This
			generates a file like shown in \autoref{fig:gcov}.

			\begin{figure} \label{fig:gcov}
				\lstinputlisting{thesis-pics/gcov.c}

				\centering \line(1,0){450}

				\lstinputlisting{thesis-pics/gcov.c.gcov}

				\caption{Example of a source code file and the .gcov file, generated by running the
				compiled program}

				\label{fig:gcov}
			\end{figure}


			Then the user can pass the \emph{gcov} generated files and the source code to the framework
			again. The framework can extract line hits from the \emph{gcov} files on the data structure
			operations and set weights on the operations according to the extracted data and then use the
			choosing algorithm for operations with weights from \autoref{sec:choose-weights}.

			It's a better solution than letting the user set the weights himself, but still, the data the
			inference is based upon comes from tests and there's no guarantee the real world data will
			match the test data.

		\subsubsection{Transforming data structures on-line} \label{sec:transforming-on-line}


	\subsection{Different element types}

		Currently the framework works only for integer elements. We could extend it to every primitive type in
		C, but it would require some changes.  Some data structures require the type to be comparable, which
		wouldn't be a big problem, because there is a comparison semantics defined on those types. There's also
		a hash function needed, because some data structures, like a hashtable, need to compute hash values to
		work. We can just use the binary representation of other types, and use it as an integer for the hash
		function. This whole modification doesn't need any user interaction.

		There's a bigger problem with composite types. Comparing pointers isn't obvious. You may want to compare
		addresses or the contents under that address, depending if you want object or structural equality. The
		same problem is with hash function arguments. There's also a problem with possible memory leaks. You can
		pass a pointer to a string to the data structure and lose the pointer in the program, then when the
		structure deletes the pointer and the string stays in the memory to the program's end. Adding this to
		the framework would require passing the comparison function, hash function and some kind of destructor
		function to the data structure, or possibly use some reference counting system. It would be very
		technical and would be beyond the scope of this thesis. With an array or a big struct passed to the
		framework, there's a problem if we want to share the data structure and just copy the pointer, copying
		the whole thing may be a bad idea, because it can have quite a lot of data inside.

		\subsubsection{Linked data structures}

			When we want to store structs like \autoref{fig:linked-struct} in a data structure, the
			framework, at the moment, could generate some data structure for this kind of structs. The
			operation on the data structure would look like on \autoref{fig:struct-old}.

			\begin{figure}
				\lstinputlisting{thesis-pics/linked-struct.c}

				\caption{An example record, that we want to store in a data structure}

				\label{fig:linked-struct}
			\end{figure}

			\begin{figure}
				\lstinputlisting{thesis-pics/struct-old.c}

				\caption{Use of the dsinf calls, to perform some data structure operations with the
				records; not very comfortable to use}

				\label{fig:struct-old}
			\end{figure}

			In \autoref{fig:struct-old} a comparison function is used, it compares the height. We can use
			any function that compares a coordinate or a combination of coordinates, but only one order of
			the elements is available per data structure. This can also be achieved without using structs at
			all, like in \autoref{fig:struct-pointer-sizeof-reduction}.

			\begin{figure}
				\lstinputlisting{thesis-pics/struct-pointer-sizeof-reduction.c}

				\caption{Encoding of the problem of having one order data structure on records in C,
				into code using pointers}

				\label{fig:struct-pointer-sizeof-reduction}
			\end{figure}

			In the \autoref{fig:struct-pointer-sizeof-reduction} above we dereference structs as arrays of
			chars, to get to any field, we just take a pointer to a field in the array and cast it to the
			right type. So it doesn't really give us any more expressiveness. It would be nice if our
			program enabled things like comparing structs by more than one condition in one program. We
			would want to be able to use operations like in \autoref{fig:struct-new}.

			\begin{figure}
				\lstinputlisting{thesis-pics/struct-new.c}

				\caption{Examples of the new dsinf API, that takes different orders on structs into
				account}

				\label{fig:struct-new}
			\end{figure}

			For us to achieve such thing, we need to change how we choose structures for record types. First
			we modify the $DSU()$ definition for record variables.
			\todo{rest}

	\subsection{Upper bound on the element count}

		When analyzing the input program, we can try to get the information, if the maximum element count in the
		data structure is bounded by some constant. If we can obtain such an information (for most programs it
		probably will be undecidable), we can use it to enhance the data structure generated for the program.

		When the element count in the data structure is potentially infinite, we have to create an
		implementation of the structure, that allocates memory lazily, when it needs the space for new elements.
		We can imagine that a structure can allocate one chunk on every insertion and free a chunk on every
		deletion, or even allocate twice as big chunks and amortize the number of allocations to $O(lg n)$.

		When we have the information about the element count, we can allocate the whole static buffer for the
		data structure and this removes the whole cost of allocation during insertions and deletions.

		Using the technique described in \autoref{sec:transforming-on-line} we can transform our dynamic
		allocating data structure in a static one, if we are in a place in the program, from which we can't
		insert any more elements. This is an example of such program:

		\begin{figure}
			\lstinputlisting{thesis-pics/upper-bound-transform.c}

			\caption{An example showing a situation, where beneficial would be to transform a dynamic data
			structure into a similar structure, but with all memory statically allocated already}

			\label{fig:upper-bound-transform}
		\end{figure}

		In \autoref{fig:upper-bound-transform} after the first loop, there are no more insertions into the data
		structure, so we can mark this spot and when we the control goes to that place, we transform the data
		structure into a static version of it, allocating a big chunk of memory and deallocating all that was
		left. This way to the end of the program we won't have to do any allocations.

	\subsection{Minimal element count threshold}

		It's worth noticing that we compare only the asymptotic complexity of data structures. Some awfully
		complicated structures can have good asymptotic results, but the constant is quite high. We can avoid
		this problem by setting a threshold for each structure, what is the smallest number of elements to use
		this data structure.

		Another problem arises, how to know at compile time, how many elements a data structure will have at
		runtime. We can ask the user to explicitly specify the number during compilation or we can try to
		detect how big the declared data is, with some kind of constant folding analysis, that checks if the
		insertion to the data structure is in a loop that is run more times than the threshold. This is a fragile
		solution, because not a lot of programs are easy to analyze this way.

		Another solution is runtime tallying the size, and using the method of transforming data structures
		on-line described in \autoref{sec:transforming-on-line}. When we hit a threshold for a data structure
		with a bigger constant, but overall better fitting to our task, we transform the old one into this one.

		\begin{figure}
			\lstinputlisting{thesis-pics/minimal-elem-count.c}

			\caption{An example showing a situation, where beneficial would be to transform a data structure
				into a more complex one, because the number of elements is sufficiently big, so the
				whole transformation is profitable}

			\label{fig:minimal-elem-count}
		\end{figure}

        In \autoref{fig:minimal-elem-count} we see a code example that would yield some kind of a priority queue in our
        framework, although it only has a few elements, and some of the fastest priority queues are pretty heavy-weight
        and it would be better just to use some simpler data structure, because the setup cost isn't amortized by the
        operations.

	\subsection{Generic data structure modifications} \label{sub:gdsm}

		When searching for the most apt data structure, we need to have some kind of data structure database,
		where we keep all the structures' metadata and implementations for the framework to use. Ideally there
		would be a lot of different data structures there.

		When implementing a data structure, one could easily modify the implementation to maybe match some rare
		specific task, that is not needed in most cases. It would be wasteful to keep a copy of the data
		structure for each small combination of those modifications, if we want to return a data structure
		tailored perfectly to the task. Of course some of the modifications are very data structure specific, so
		a generalization isn't possible, but for some cases, we can extract a piece of code, like a wrapper for
        a data structure, that modifies its behavior in some specific way (similiar pattern was used in
        \cite{Okasaki}).

		\subsubsection{Extremal element cache}

			When we want to be able to lookup extremal elements in the data structure. We don't really have
			to know the implementation specifics of the data structure, we only need to intercept the calls
			which insert and delete elements. When we insert an element, we compare it to a cache variable,
			which keeps the biggest element (if our extremal element is the maximal element) known to be in
			the data structure.

			But this road also has some drawbacks, if we delete an element, and it occurs that the deleted
			element is the current maximum, we have to find the new maximal element from the rest of
			elements, which can be asymptoticly more costly than the delete itself.

			\missingfigure{table of cost changes}

			\begin{figure}
				\lstinputlisting{thesis-pics/elem-cache.c}

				\caption{An example implementation of the extremal element cache data structure wrapper}

				\label{fig:elem-cache}
			\end{figure}

			We notice that a lot of data structures already implement this pattern \cite{Wiveb}.

        \subsubsection{Linked elements}\label{sec:gdsm:le}

			When we have a structure that keeps some data, to find a predecessor or a successor, it usually
			takes $O(n)$ or $O(lg n)$ operations.  To help this we can create an overlay with a doubly
			linked list on the elements, so our predecessor/successor lookups are $O(1)$. Drawback here is
			that the cost of insert is increased by searching the successor to link it appropriately.

			\begin{figure}
				\lstinputlisting{thesis-pics/linked-elements.c}

				\caption{An example implementation of the linked elements data structure wrapper}

				\label{fig:linked-cache}
			\end{figure}

\section{Program}

	The framework has a few modes of working:
	\begin{itemize}
		\item Recommendation mode - the framework analyzes the input files and the data structure operations in
			them, then outputs the recommended data structure on standard output
		\item Advice mode - the framework checks if any small change in the data structure operations would give
			the program a speed boost
		\item Compile mode - works as the Recommendation mode, but instead of printing the data structure to
			standard output, it compiles the files and links it with the chosen structure implementation.
	\end{itemize}

\section{Future work} \label{sec:future}
	It may be a good idea to rewrite the framework to analyze LLVM\cite{LLVM} IR internally, instead of the C
	language AST. This could yield better results because of the optimizations that clang\cite{Clang} can apply to
	the initial code, like removing unnecessary calls to data structure operations, so the framework can infer the
	data structure based on the optimized code. LLVM also enables a link-time optimization, so after linking the
	chosen data structure to a program, another optimization step is performed. Also analyzing LLVM IR is easier
	that analyzing the C language.

\begin{thebibliography}{9}
	\bibitem{Wids} Wikipedia - http://en.wikipedia.org/wiki/Data\_structure
    \bibitem{Wipgo} Wikipedia - http://en.wikipedia.org/wiki/Profile-guided\_optimization
    \bibitem{Wiveb} Wikipedia - http://en.wikipedia.org/wiki/Van\_Emde\_Boas\_tree\#cite\_note-3
	\bibitem{LLVM} The LLVM Compiler Infrastructure - http://llvm.org/
	\bibitem{Okasaki} Purely Functional Data Structures - Chris Okasaki
	\bibitem{Clang} clang: a C language family frontend for LLVM - http://clang.llvm.org/
	\bibitem{AppleCC} Cocoa Core Competencies - Class Clusters - \\
		https://developer.apple.com/library/mac/\#documentation/General/Conceptual/DevPedia-CocoaCore/ClassCluster.html
\end{thebibliography}


\end{document}
% vim: set wrap tw=120:
